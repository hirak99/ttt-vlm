{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed764e3",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss\n",
    "\n",
    "Suppose for a batch, we have target values $[v_1, ..., v_n]$.\n",
    "\n",
    "Then probabilities are defined as **softmax**, or $p_k = \\frac{e^{v_k}}{\\sum_k{e^{v_k}}}$.\n",
    "\n",
    "Overall loss for the batch is $l = \\sum_k {-y_k \\log p_k}$.\n",
    "\n",
    "Note:\n",
    "- $y_k$'s do not necessarily need to be $1$'s and $0$'s. But they often are for known labels with one-hot encoding.\n",
    "- The loss is not symmetric wrt. $y$'s and $p$'s.\n",
    "\n",
    "## Notes on Operating\n",
    "\n",
    "For experiments see the relevant sections below.\n",
    "\n",
    "### Class IDs\n",
    "\n",
    "The targets could also just be integers denoting the class ids.\n",
    "\n",
    "### Multi-Target\n",
    "\n",
    "Suppose you simultaneously want multiple independent targets to be inferred.\n",
    "\n",
    "For example, you want different cells each with labels 'X', 'O' or '.'.\n",
    "\n",
    "Then you must arrange your data thus -\n",
    "\n",
    "- Dimension 1: Batch\n",
    "- Dimention 2: Class\n",
    "- Dimention 3 onwards: All independent sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1077ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8269b",
   "metadata": {},
   "source": [
    "## Generate Data for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56878208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269,  1.4873,  0.9007],\n",
      "        [-2.1055,  0.6784, -1.2345],\n",
      "        [-0.0431, -1.6047, -0.7521],\n",
      "        [ 1.6487, -0.3925, -1.4036],\n",
      "        [-0.7279, -0.5594, -2.3169],\n",
      "        [-0.2168, -1.3847, -0.8712],\n",
      "        [-0.2234,  1.7174,  0.3189],\n",
      "        [-0.4245, -0.8286,  0.3309],\n",
      "        [-1.5576,  0.9956, -0.8798],\n",
      "        [-0.6011, -1.2742,  2.1228]], requires_grad=True)\n",
      "tensor([[0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Normally _ROWS denote # batches.\n",
    "_ROWS = 10\n",
    "# _COLS denote all the classes.\n",
    "_COLS = 3\n",
    "\n",
    "input = torch.randn((_ROWS, _COLS), requires_grad=True)\n",
    "print(input)\n",
    "\n",
    "target = torch.zeros((_ROWS, _COLS))\n",
    "for index in range(_ROWS):\n",
    "    target[index, torch.randint(_COLS, (1,))] = 1\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223742b1",
   "metadata": {},
   "source": [
    "## Dissecting the Cross Entropy implementation in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c2f75fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2496, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "loss_unreduced = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "loss(input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4103c7",
   "metadata": {},
   "source": [
    "It is the same as computing **per batch, and then taking the mean**.\n",
    "\n",
    "NB. This behavior can be controlled with the parameter `reduction` which defaults to `\"mean\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39c73c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2496, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss(input[i, :], target[i, :]) for i in range(_ROWS)) / _ROWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20df4ec",
   "metadata": {},
   "source": [
    "We can check by computing it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc254ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(1.2496029)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def alt_defn(input, target):\n",
    "    with torch.no_grad():\n",
    "        # Softmax.\n",
    "        # This could also be computed as -\n",
    "        # probs = torch.softmax(input, dim=1)\n",
    "        exp = np.exp(input.numpy())\n",
    "        probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "        per_batch_loss = (-target.numpy() * np.log(probs)).sum(axis=1)\n",
    "        return per_batch_loss.mean()\n",
    "\n",
    "\n",
    "alt_defn(input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96bfa6",
   "metadata": {},
   "source": [
    "### Class ID Instead of One-Hot\n",
    "\n",
    "Instead of one-hot, we could also directly pass the class ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0870028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 2, 0, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_ids = target.argmax(dim=1)\n",
    "class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bedbce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2496, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(input, class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdae559",
   "metadata": {},
   "source": [
    "## Multi Target\n",
    "\n",
    "Note that just changing the view wouldn't work.\n",
    "\n",
    "You will also have to ensure that dim 1 is the class. This can be done by transposing, and we see the loss indeed matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14b9f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_m = input.view(2, 5, 3).transpose(1, 2)\n",
    "target_m = target.view(2, 5, 3).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4713d558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2496, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(input_m, target_m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
